{"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"CnvOnhAVM18T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdGTESforjZS"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","source":["!pip install numpy pandas matplotlib seaborn plotly scipy statsmodels scikit-learn xgboost category_encoders wandb optuna lightgbm catboost tensorflow tensorboard shap dask"],"metadata":{"id":"4rZxxtsGhye_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEFn3WOErenN"},"outputs":[],"source":["# TODO: Change path to your actual direction\n","\n","%cd '/content/drive/MyDrive/Colab Notebooks/[Dreamweavers] Workspace/Round 2_RBAC 2024/Xuân Thành/submit_code'\n","loan_df_path = '/content/drive/MyDrive/Colab Notebooks/[Dreamweavers] Workspace/Round 2_RBAC 2024/[RBAC 2024] Case Study Package/dataset/loan_origin.csv'\n","demographic_df_path = '/content/drive/MyDrive/Colab Notebooks/[Dreamweavers] Workspace/Round 2_RBAC 2024/[RBAC 2024] Case Study Package/dataset/demographic.csv'\n","\n","loan_df = pd.read_csv(loan_df_path)\n","demographic_df = pd.read_csv(demographic_df_path)\n","\n","\n","joined_df_path = 'joined_dataset.csv'\n","joined_df_remap_path = 'joined_dataset_remap.csv'\n","\n","from cleaner.check_data import *\n","from cleaner.remove_dup import *\n","from cleaner.standardize import *\n","from cleaner.impute import *\n","from cleaner.outlier import *\n","from cleaner.consistency import *\n","from cleaner.remap_values import *"]},{"cell_type":"markdown","source":["# Clean Data"],"metadata":{"id":"jL8gOiGV-xZh"}},{"cell_type":"code","source":["understand_df(loan_df, output_dir=\"demographic\")\n","loan_quality_issues = validate_data_quality(loan_df, \"Demographic Dataset\")"],"metadata":{"id":"ls8liCM_A5SX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["understand_df(loan_df, output_dir=\"loan_origin\")\n","loan_quality_issues = validate_data_quality(loan_df, \"Loan Dataset\")"],"metadata":{"id":"c-6JQFLBMvY_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Demographic"],"metadata":{"id":"TDcMDyRVMdnf"}},{"cell_type":"code","source":["dataset_type = 'demographic'\n","logger = setup_logger()\n","logger.info(f\"Processing {dataset_type} dataset\")\n","\n","demographic_df = pd.read_csv(demographic_df_path)\n","\n","# Standardize data formats\n","demographic_df = standardize_data_formats(demographic_df, dataset_name=dataset_type)\n","\n","# Identify and handle duplicates\n","duplicate_summary = identify_duplicates(demographic_df, logger)\n","demographic_df = handle_duplicates(demographic_df, duplicate_summary, logger)\n","\n","# Normalize missing values\n","demographic_df = normalize_missing_values(demographic_df)\n","# print('after normalized: ',df.head)\n","\n","# Initialize and apply advanced imputer\n","imputer = AdvancedImputer(logger)\n","demographic_df_imputed = imputer.fit_transform(demographic_df)\n","\n","# Apply domain-specific rules\n","rules = get_domain_specific_imputation_rules()\n","demographic_df_imputed = apply_domain_specific_rules(demographic_df_imputed, rules, logger)\n","\n","# Apply constraints\n","demographic_df_imputed = apply_constraints(demographic_df_imputed)\n","\n","# Final cleaning: Fill any remaining missing values\n","demographic_df_imputed.fillna(method='ffill', inplace=True)\n","demographic_df_imputed.fillna(method='bfill', inplace=True)\n","\n","# Identify and handle outliers\n","outlier_summary = detect_outliers(demographic_df_imputed)\n","demographic_df,_ = handle_outliers(demographic_df_imputed, outlier_summary, strategy='cap')\n","\n","# Check for consistency\n","remove_exact_duplicates(demographic_df, 'contract_no')\n","demographic_df = check_consistency(demographic_df, logger, dataset_type)\n","\n","logger.info(\"Dataset processing completed\")"],"metadata":{"id":"Mb96o0i4tpT-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loan Origin"],"metadata":{"id":"gLjeKvA1MfxA"}},{"cell_type":"code","source":["dataset_type = 'loan_origin'\n","logger = setup_logger()\n","logger.info(f\"Processing {dataset_type} dataset\")\n","\n","loan_df = pd.read_csv(loan_df_path)\n","\n","# Standardize data formats\n","loan_df = standardize_data_formats(loan_df, dataset_name=dataset_type)\n","\n","# Identify and handle duplicates\n","duplicate_summary = identify_duplicates(loan_df, logger)\n","loan_df = handle_duplicates(loan_df, duplicate_summary, logger)\n","\n","# Normalize missing values\n","loan_df = normalize_missing_values(loan_df)\n","# print('after normalized: ',df.head)\n","\n","# Initialize and apply advanced imputer\n","imputer = AdvancedImputer(logger)\n","loan_df_imputed = imputer.fit_transform(demographic_df)\n","\n","# Apply domain-specific rules\n","rules = get_domain_specific_imputation_rules()\n","loan_df_imputed = apply_domain_specific_rules(loan_df_imputed, rules, logger)\n","\n","# Apply constraints\n","loan_df_imputed = apply_constraints(loan_df_imputed)\n","\n","# Final cleaning: Fill any remaining missing values\n","loan_df_imputed.fillna(method='ffill', inplace=True)\n","loan_df_imputed.fillna(method='bfill', inplace=True)\n","\n","# Identify and handle outliers\n","outlier_summary = detect_outliers(loan_df_imputed)\n","loan_df,_ = handle_outliers(loan_df_imputed, outlier_summary, strategy='cap')\n","\n","# Check for consistency\n","remove_exact_duplicates(loan_df, 'contract_no')\n","loan_df = check_consistency(loan_df, logger, dataset_type)\n","\n","logger.info(\"Dataset processing completed\")"],"metadata":{"id":"sMw3o9iI2LaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Join Dataset"],"metadata":{"id":"h2oC4S-SMh05"}},{"cell_type":"code","source":["def inner_join_datasets(loan_df, demographic_df, on_column='contract_no'):\n","  \"\"\"\n","  Performs an inner join on two datasets based on the specified column.\n","\n","  Args:\n","    loan_df_path (str): Path to the loan origin dataset CSV file.\n","    demographic_df_path (str): Path to the demographic dataset CSV file.clear\n","    on_column (str): Column name to join the datasets on.\n","\n","  Returns:\n","    pd.DataFrame: The joined dataset.\n","  \"\"\"\n","  try:\n","    joined_df = pd.merge(loan_df, demographic_df, on=on_column, how='inner')\n","    remove_exact_duplicates(joined_df, 'contract_no')\n","    if joined_df is not None:\n","      joined_df.to_csv('joined_dataset.csv', index=False)\n","      print(joined_df.head())\n","      print(joined_df.sample(5))\n","      print(\"Joined dataset saved as 'joined_dataset.csv'\")\n","      return joined_df\n","    else:\n","      print(\"Error: Joined dataset is empty.\")\n","\n","  except FileNotFoundError as e:\n","    print(f\"Error: One or both of the input files not found. {e}\")\n","    return None\n","  except Exception as e:\n","    print(f\"Error during inner join: {e}\")\n","    return None\n","\n","inner_join_datasets(loan_df, demographic_df)\n","joined_df = pd.read_csv(joined_df_path)"],"metadata":{"id":"NEaAeWrf3XKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_correlation_matrix(df, title=\"Correlation Matrix Heatmap\", save_path=None):\n","    \"\"\"\n","    Plots a correlation matrix heatmap of numerical features,\n","    excluding 'contract_no'.\n","\n","    Args:\n","        df: The input DataFrame.\n","        title: The title of the plot.\n","        save_path: Optional; path to save the plot as an image file.\n","    \"\"\"\n","    # Exclude 'contract_no' and select only numerical features\n","    numerical_features = df.select_dtypes(include=np.number).drop(columns=['contract_no'], errors='ignore').columns\n","    correlation_matrix = df[numerical_features].corr()\n","\n","    plt.figure(figsize=(12, 10))\n","    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=.5)\n","    plt.title(title, fontsize=16)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","\n","    # Save the figure if a save_path is provided\n","    if save_path:\n","        plt.savefig(save_path, dpi=600)\n","        plt.close()\n","\n","    # plt.show()\n","\n","plot_correlation_matrix(joined_df, save_path='correlation_matrix.png')"],"metadata":{"id":"vOnZD3Mhp1cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQL41qd2v120"},"source":["# Analyse Questions"]},{"cell_type":"code","source":["from question_support import *\n","run_analysis(joined_df_path)"],"metadata":{"id":"8iNJYvogBC3p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"1Zg62LviLUmy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKi-RbaF3dY3"},"outputs":[],"source":["joined_df = pd.read_csv(joined_df_path)\n","# joined_df.head()\n","joined_df.info()\n","# joined_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwGS0IE-3gcG"},"outputs":[],"source":["def plot_correlation_matrix(df, title=\"Correlation Matrix Heatmap\", save_path=None):\n","    \"\"\"\n","    Plots a correlation matrix heatmap of numerical features,\n","    excluding 'contract_no'.\n","\n","    Args:\n","        df: The input DataFrame.\n","        title: The title of the plot.\n","        save_path: Optional; path to save the plot as an image file.\n","    \"\"\"\n","    # Exclude 'contract_no' and select only numerical features\n","    numerical_features = df.select_dtypes(include=np.number).drop(columns=['contract_no'], errors='ignore').columns\n","    correlation_matrix = df[numerical_features].corr()\n","\n","    plt.figure(figsize=(12, 10))\n","    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=.5)\n","    plt.title(title, fontsize=16)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","\n","    # Save the figure if a save_path is provided\n","    if save_path:\n","        plt.savefig(save_path, dpi=600)\n","        plt.close()\n","\n","    # plt.show()\n","\n","plot_correlation_matrix(joined_df, save_path='correlation_matrix.png')"]},{"cell_type":"markdown","source":["# Modelling"],"metadata":{"id":"MdlL-IviMzBU"}},{"cell_type":"markdown","source":["## Time-Series Classification and Regression"],"metadata":{"id":"J0x6w8ISNB4N"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n","import xgboost as xgb\n","import lightgbm as lgb\n","import optuna\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import VarianceThreshold\n","\n","output_file = 'model_evaluation_results.txt'\n","models_dir = 'models'\n","os.makedirs(models_dir, exist_ok=True)\n","\n","def preprocess_data(joined_df):\n","    \"\"\"Preprocess the dataset for combined-year modeling with time-aware splits.\"\"\"\n","    joined_df['disbursement_date'] = pd.to_datetime(joined_df['disbursement_date'], errors='coerce')\n","    joined_df['disbursement_year'] = joined_df['disbursement_date'].dt.year\n","\n","    # Identify categorical columns and apply one-hot encoding\n","    categorical_columns = joined_df.select_dtypes(include='object').columns\n","    joined_df = pd.get_dummies(joined_df, columns=categorical_columns, drop_first=True)\n","\n","    # Define target columns after encoding\n","    product_category_columns = [col for col in joined_df.columns if 'product_category_' in col]\n","\n","    # Drop unnecessary columns\n","    columns_to_drop = ['disbursement_date', 'age']\n","    for col in columns_to_drop:\n","        if col in joined_df.columns:\n","            joined_df = joined_df.drop(columns=[col])\n","\n","    # Standardize numerical features\n","    numerical_features = ['loan_amount', 'customer_income', 'insurance_rate', 'month_interest', 'weight', 'height']\n","    scaler = StandardScaler()\n","    joined_df[numerical_features] = scaler.fit_transform(joined_df[numerical_features])\n","\n","    # Remove low-variance features\n","    X = joined_df.drop(columns=product_category_columns)\n","    X = drop_low_variance_features(X)\n","    y = joined_df[product_category_columns]\n","    y_loan = joined_df['loan_amount']\n","    y_rate = joined_df['rate']\n","\n","    return X, y, y_loan, y_rate, product_category_columns\n","\n","def drop_low_variance_features(df, threshold=0.01):\n","    selector = VarianceThreshold(threshold)\n","    selector.fit(df)\n","    return df[df.columns[selector.get_support(indices=True)]]\n","\n","def tune_lightgbm(X, y):\n","    \"\"\"Hyperparameter tuning for LightGBM using Optuna with time-series cross-validation.\"\"\"\n","    def objective(trial):\n","        param = {\n","            'objective': 'binary',\n","            'metric': 'auc',\n","            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n","            'max_depth': trial.suggest_int('max_depth', 3, 10),\n","            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n","            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n","            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n","            'is_unbalance': True,\n","            'min_gain_to_split': 0.01\n","        }\n","\n","        model = MultiOutputClassifier(lgb.LGBMClassifier(**param))\n","        tscv = TimeSeriesSplit(n_splits=5)\n","        aucs = []\n","\n","        for train_index, valid_index in tscv.split(X):\n","            X_tr, X_val = X.iloc[train_index], X.iloc[valid_index]\n","            y_tr, y_val = y.iloc[train_index], y.iloc[valid_index]\n","            model.fit(X_tr, y_tr)\n","            y_val_pred_proba = model.predict_proba(X_val)\n","\n","            fold_aucs = []\n","            for i in range(y_val.shape[1]):\n","                positive_class_probs = y_val_pred_proba[i][:, 1]\n","                auc = roc_auc_score(y_val.iloc[:, i], positive_class_probs)\n","                fold_aucs.append(auc)\n","\n","            aucs.append(np.mean(fold_aucs))\n","\n","        return np.mean(aucs)\n","\n","    study = optuna.create_study(direction='maximize')\n","    study.optimize(objective, n_trials=50)\n","    best_params = study.best_params\n","    print(\"Best parameters from Optuna:\", best_params)\n","    return best_params\n","\n","def build_nn(input_shape):\n","    \"\"\"Create a neural network model for regression.\"\"\"\n","    model = models.Sequential([\n","        layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n","        layers.Dense(64, activation='relu'),\n","        layers.Dense(32, activation='relu'),\n","        layers.Dense(1)\n","    ])\n","    model.compile(optimizer='adam', loss='mse')\n","    return model\n","\n","def train_nn_loan_model(X_train, y_train, X_test, y_test):\n","    nn_loan = build_nn(X_train.shape[1])\n","    tensorboard_callback = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n","    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)\n","    nn_loan.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[tensorboard_callback, early_stopping_callback])\n","    nn_loan.save(os.path.join(models_dir, 'nn_loan_model.h5'))\n","    nn_loan_preds = nn_loan.predict(X_test)\n","    print(\"Neural Network RMSE for Loan Amount:\", mean_squared_error(y_test, nn_loan_preds, squared=False))\n","    return nn_loan\n","\n","def evaluate_classification_model(y_test, y_pred, y_pred_proba, product_category_columns):\n","    auc_scores = []\n","    with open(output_file, 'a') as f:\n","        for i, col in enumerate(product_category_columns):\n","            positive_class_probs = y_pred_proba[i][:, 1]\n","            auc = roc_auc_score(y_test[col], positive_class_probs)\n","            auc_scores.append(auc)\n","            f.write(f\"AUC for {col}: {auc}\\n\")\n","\n","        accuracy_scores = [accuracy_score(y_test.iloc[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n","        f.write(\"Multi-Label Accuracy for each product category: \" + str(accuracy_scores) + \"\\n\")\n","\n","def evaluate_regression_model(y_test, y_preds, metric_name):\n","    rmse = mean_squared_error(y_test, y_preds, squared=False)\n","    with open(output_file, 'a') as f:\n","        f.write(f\"{metric_name} RMSE: {rmse}\\n\")\n","\n","joined_df = pd.read_csv(joined_df_path)\n","\n","# Preprocess the data\n","X, y, y_loan, y_rate, product_category_columns = preprocess_data(joined_df)\n","\n","# TimeSeriesSplit for cross-validation\n","tscv = TimeSeriesSplit(n_splits=5)\n","\n","train_accuracies, test_accuracies = [], []\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    y_loan_train, y_loan_test = y_loan.iloc[train_index], y_loan.iloc[test_index]\n","\n","    # Tune LightGBM for classification\n","    best_lgbm_params = tune_lightgbm(X_train, y_train)\n","    base_model = lgb.LGBMClassifier(**best_lgbm_params)\n","    multi_target_model = MultiOutputClassifier(base_model)\n","    multi_target_model.fit(X_train, y_train)\n","    y_pred_proba = multi_target_model.predict_proba(X_test)\n","    y_pred = multi_target_model.predict(X_test)\n","\n","    # Save each LightGBM model for each target label\n","    for i, estimator in enumerate(multi_target_model.estimators_):\n","        booster = estimator.booster_\n","        booster.save_model(os.path.join(models_dir, f'lgbm_classifier_model_target_{i}_fold_{fold}.txt'))\n","\n","    # Evaluate and save model results\n","    train_accuracy = multi_target_model.score(X_train, y_train)\n","    test_accuracy = multi_target_model.score(X_test, y_test)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    evaluate_classification_model(y_test, y_pred, y_pred_proba, product_category_columns)\n","\n","    # Train and evaluate Gradient Boosting Regressor for Loan Amount\n","    gbr_loan = xgb.XGBRegressor(learning_rate=0.05, n_estimators=200, max_depth=7, subsample=0.8)\n","    gbr_loan.fit(X_train, y_loan_train)\n","    loan_preds = gbr_loan.predict(X_test)\n","    evaluate_regression_model(y_loan_test, loan_preds, \"Gradient Boosting\")\n","    gbr_loan.save_model(os.path.join(models_dir, f'xgb_regressor_model_fold_{fold}.json'))\n","\n","    # Train and evaluate Neural Network for Loan Amount\n","    nn_loan = train_nn_loan_model(X_train, y_loan_train, X_test, y_loan_test)\n","\n","# Plot training and validation accuracy across folds\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n","plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Validation Accuracy')\n","plt.xlabel('Fold')\n","plt.ylabel('Accuracy')\n","plt.title('LightGBM Classifier Accuracy During Cross-Validation')\n","plt.legend()\n","plt.savefig(os.path.join(models_dir, 'training_validation_accuracy.png'))\n","plt.show()"],"metadata":{"id":"w-3S1PYxM0Oq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RandomForest & Decision Tree Classification"],"metadata":{"id":"PAgwDofBNbJC"}},{"cell_type":"code","source":["def feature_importance_examination():\n","    df = joined_df\n","    question_3_data = df[(df['product_category'] == 0) | (df['product_category'] == 1) | (df['product_category'] == 2)]\n","    question_3_data.drop(['contract_no', 'permanent_address_province'], axis=1, inplace=True)\n","\n","    scaler_1 = StandardScaler()\n","    question_3_data_scaled = scaler_1.fit_transform(question_3_data)\n","\n","    y = question_3_data['product_category']\n","    X = question_3_data.drop(['product_category', 'creditibility'], axis=1)\n","\n","    model = LogisticRegression(max_iter=1000)\n","    model.fit(X, y)\n","    y_pred = model.predict(X)\n","    feature_names = X.columns\n","    coefficients = model.coef_\n","\n","    feature_importance = pd.DataFrame(coefficients.T, index=feature_names, columns=model.classes_)\n","    feature_importance['Absolute Importance'] = feature_importance.abs().max(axis=1)\n","    feature_importance = feature_importance.sort_values(by='Absolute Importance', ascending=False)\n","\n","    print(\"Feature Importance for Each Class:\")\n","    print(feature_importance[['Absolute Importance'] + list(model.classes_)])\n","\n","    model_1 = RandomForestClassifier()\n","    model_1.fit(X, y)\n","    y_pred = model_1.predict(X)\n","\n","    feature_importances = model_1.feature_importances_\n","    feature_importance_df = pd.DataFrame({\n","        'Feature': feature_names,\n","        'Importance': feature_importances\n","    })\n","    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n","    print(\"Feature Importance in Random Forest Classifier:\")\n","    print(feature_importance_df)\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n","    plt.xlabel('Importance')\n","    plt.ylabel('Feature')\n","    plt.title('Feature Importance in Random Forest Classifier')\n","    plt.gca().invert_yaxis()\n","    plt.show()\n","\n","feature_importance_examination()"],"metadata":{"id":"ybOdiSn8Nc6J"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}